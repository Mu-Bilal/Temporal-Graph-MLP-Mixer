project: GMM-1  # For WandB
seed: null  # Whether fix the running seed to remove randomness

raw_data:  # HDTTS data pipeline settings
  name: la_normal  # dataset_failureMode
  mode:  # Part of missing data algo. Merged with configs/mode
    seed: 207  # 9101112
    min_seq: 12  # 1 hour
    max_seq: 48  # 4 hours
  connectivity:
    method: distance
    threshold: 0.1
    include_self: False
    layout: csr
  make_graph_connected: False
  hparams: {}

dataset:
  max_len: null
  train_size: 0.7
  val_size: 0.1
  horizon: 12
  window: 12
  stride: 1
  delay: 0
  EngRADchannel: 0
  normalize: True

train:
  batch_size: 16  # Total graph mini-batch size
  epochs: 1  # Maximal number of epochs

  mask_loss: True  # If this is True, imputed values are ignored when computing prediction loss (independent of metrics, see `logging` below)

  lr: 1e-3  # Base learning rate
  lr_patience: 10  # number of steps before reduce learning rate
  lr_decay: 0.5  # learning rate decay factor
  min_lr: 1.0e-5  # A lower bound on the learning rate.

  early_stop_patience: 5

  monitor: valid/loss  # Metric to monitor

  wd: 2e-1  # L2 regularization, weight decay
  dropout_gnn: 0.8  # Dropout rate
  dropout_readout: 0.7  # Dropout rate for Readout
  dropout_patch_mixer: 0.7  # Dropout rate for patch MLPMixer
  dropout_node_mixer: 0.6  # Dropout rate for node MLPMixer

model:
  gnn_type: GINEConv  # GNN type used, see core.model_utils.pyg_gnn_wrapper for all options
  gMHA_type: MLPMixer  # GraphMLPMixer or graph-based multihead attention: [MLPMixer, Hadamard, Standard, Graph, Addictive, Kernel]
  
  nfeatures_patch: 128  # Number of features for patch mixer
  nfeatures_node: 64  # Number of features for node mixer

  nlayer_gnn: 2  # Number of gnn layers  FIXME: Check if receptive field is enough
  nlayer_patch_mixer: 2  # Number of mlp mixer layers
  nlayer_node_mixer: 3 # Number of mlp mixer layers

  nlayer_readout: 1  # Number of mlp layers for readout

  pool: mean  # Pooling type for generating graph/subgraph embedding from node embeddings
  residual: True  # Use residual connection

# Positional encoding options (currently not used directly, but needed for GraphPartitionTransform - TODO: Check and rm)
pos_enc:
  rw_dim: 16  # Random walk structural encoding
  lap_dim: 0  # Laplacian eigenvectors positional encoding
  patch_rw_dim: 8  # Patch random walk structural encoding
  patch_num_diff: -1  # Patch PE diffusion steps

# Metis patch extraction options
metis:
  enable: True  # Enable Metis partition (otherwise use random partition)
  online: False  # Enable data augmentation
  n_patches: 80  # The number of partitions
  num_hops: 1  # expanding patches with k hop neighbourhood

logging:
  log_horizons: [3, 6, 12, "all"]
  ignore_invalid: True
